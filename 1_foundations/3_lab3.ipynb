{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "+91-9742509849 (Mobile)\n",
      "vijaybrhegade@gmail.com\n",
      "www.linkedin.com/\n",
      "in/vijayamahantesh-\n",
      "hegade-03164138 (LinkedIn)\n",
      "Top Skills\n",
      "Teradata SQL\n",
      "Google Trends\n",
      "Google Search Console\n",
      "Vijayamahantesh Hegade\n",
      "Data Analytics, Data Modelling and Visualization\n",
      "Bengaluru, Karnataka, India\n",
      "Summary\n",
      "I’m a data professional with over 10 years of experience helping\n",
      "organizations build scalable data platforms, analytics solutions,\n",
      "and executive dashboards. My expertise spans data engineering,\n",
      "analytics engineering, ETL development, and BI, with strong hands-\n",
      "on experience in SQL, Python, Snowflake, dbt, Airflow, Tableau, and\n",
      "Power BI.I’ve worked across industries including financial services,\n",
      "analytics, and digital platforms, delivering end-to-end data pipelines,\n",
      "optimizing workflows, and enabling data-driven decision-making. I’m\n",
      "passionate about turning complex data into actionable insights and\n",
      "building reliable, scalable data solutions that drive business impact.\n",
      "Experience\n",
      "Brillio\n",
      "Senior Business Intelligence Specialist\n",
      "March 2024 - Present (2 years)\n",
      "Bengaluru\n",
      "Led SEO analytics and data engineering initiatives by designing scalable end-\n",
      "to-end data pipelines using SQL, Snowflake, dbt, and Airflow to process large-\n",
      "scale SERP and crawl datasets. Built curated data models and interactive\n",
      "Tableau dashboards to deliver insights on keyword rankings, crawl behavior,\n",
      "and organic performance, helping improve Share of Voice by ~3% and\n",
      "keyword rankings.\n",
      "Delivered actionable crawl analytics that improved page indexation by 3.4%\n",
      "and increased search impressions by ~16% through sitemap and crawl\n",
      "optimization strategies. Additionally, drove data platform modernization by\n",
      "automating executive reporting and migrating 20+ dashboards to Snowflake\n",
      "and Tableau, improving performance, scalability, and reporting reliability.\n",
      "Luxoft\n",
      "Lead Engineer\n",
      "January 2022 - March 2024 (2 years 3 months)\n",
      "Bengaluru\n",
      "  Page 1 of 3   \n",
      "Analyzed financial transactions across multiple SWIFT message types to\n",
      "support AML and compliance monitoring, building scalable data pipelines\n",
      "using HDFS and Hive. Developed 10+ Tableau dashboards delivering key\n",
      "insights and implemented RBAC for 50+ users to ensure secure, compliant\n",
      "data access.\n",
      "State Street\n",
      "2 years 5 months\n",
      "Officer\n",
      "April 2021 - December 2021 (9 months)\n",
      "Bangalore Urban\n",
      "Designed and developed FP&A dashboards using Tableau and Power BI\n",
      "to support financial decision-making, automated 5+ legacy Excel reports\n",
      "to improve accuracy and efficiency, managed RBAC for global users, and\n",
      "provided production support to ensure reliable reporting performance.\n",
      "Senior Associate\n",
      "August 2019 - March 2021 (1 year 8 months)\n",
      "Bangalore Urban, Karnataka, India\n",
      "Enquero\n",
      "Engineer -Data Analysis and Visualization\n",
      "February 2019 - August 2019 (7 months)\n",
      "Bangalore\n",
      "Developed and maintained SQL-based ETL workflows to populate EDW\n",
      "tables with automated scheduling, enhanced dashboards by integrating new\n",
      "KPIs while maintaining performance, and provided production support by\n",
      "troubleshooting data issues and implementing enhancements.\n",
      "Tata Consultancy Services\n",
      "6 years 5 months\n",
      "IT Analyst\n",
      "October 2016 - February 2019 (2 years 5 months)\n",
      "Bangalore\n",
      "Designed and developed ETL workflows and EDW data pipelines using\n",
      "SSIS, Informatica, SQL, and Teradata, ensuring data quality and optimizing\n",
      "performance through workflow tuning. Built and optimized Tableau dashboards\n",
      "and managed Tableau Server administration, including RBAC and site\n",
      "governance, to deliver scalable and reliable reporting solutions at American\n",
      "Express.\n",
      "  Page 2 of 3   \n",
      "Software Engineer\n",
      "October 2012 - September 2016 (4 years)\n",
      "Bengaluru, Karnataka, India\n",
      "Education\n",
      "B. V. Bhoomaraddi College of Engg. & Tech., Hubli\n",
      "Engineer's Degree, Computer Science · (2008 - 2012)\n",
      "Karnataka Science College, Dharwad\n",
      "Pre University (PUC), Physics, Chemistry, Mathematics,\n",
      "Biology · (2006 - 2008)\n",
      "Karnataka High School, Dharwad\n",
      "10, English, Sanskrit, Kannada, Mathematics, Science · (2000 - 2006)\n",
      "Vidyaranya Primary School, Dharwad\n",
      "Grade 4, Kannada, Basic Mathematics, Basic Science · (1996 - 2000)\n",
      "  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"vhegade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as vhegade. You are answering questions on vhegade's website, particularly questions related to vhegade's career, background, skills and experience. Your responsibility is to represent vhegade for interactions on the website as faithfully as possible. You are given a summary of vhegade's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nI’m a data professional with over 10 years of experience helping organizations build scalable data platforms, analytics solutions, and executive dashboards.\\nMy expertise spans data engineering, analytics engineering, ETL development, and BI, with strong handson experience in SQL, Python, Snowflake, dbt, Airflow, Tableau, and Power BI.\\nI’ve worked across industries including financial services, analytics, and digital platforms, delivering end-to-end data pipelines, optimizing workflows, and enabling data-driven decision-making. \\nI’m passionate about turning complex data into actionable insights and building reliable, scalable data solutions that drive business impact.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n+91-9742509849 (Mobile)\\nvijaybrhegade@gmail.com\\nwww.linkedin.com/\\nin/vijayamahantesh-\\nhegade-03164138 (LinkedIn)\\nTop Skills\\nTeradata SQL\\nGoogle Trends\\nGoogle Search Console\\nVijayamahantesh Hegade\\nData Analytics, Data Modelling and Visualization\\nBengaluru, Karnataka, India\\nSummary\\nI’m a data professional with over 10 years of experience helping\\norganizations build scalable data platforms, analytics solutions,\\nand executive dashboards. My expertise spans data engineering,\\nanalytics engineering, ETL development, and BI, with strong hands-\\non experience in SQL, Python, Snowflake, dbt, Airflow, Tableau, and\\nPower BI.I’ve worked across industries including financial services,\\nanalytics, and digital platforms, delivering end-to-end data pipelines,\\noptimizing workflows, and enabling data-driven decision-making. I’m\\npassionate about turning complex data into actionable insights and\\nbuilding reliable, scalable data solutions that drive business impact.\\nExperience\\nBrillio\\nSenior Business Intelligence Specialist\\nMarch 2024\\xa0-\\xa0Present\\xa0(2 years)\\nBengaluru\\nLed SEO analytics and data engineering initiatives by designing scalable end-\\nto-end data pipelines using SQL, Snowflake, dbt, and Airflow to process large-\\nscale SERP and crawl datasets. Built curated data models and interactive\\nTableau dashboards to deliver insights on keyword rankings, crawl behavior,\\nand organic performance, helping improve Share of Voice by ~3% and\\nkeyword rankings.\\nDelivered actionable crawl analytics that improved page indexation by 3.4%\\nand increased search impressions by ~16% through sitemap and crawl\\noptimization strategies. Additionally, drove data platform modernization by\\nautomating executive reporting and migrating 20+ dashboards to Snowflake\\nand Tableau, improving performance, scalability, and reporting reliability.\\nLuxoft\\nLead Engineer\\nJanuary 2022\\xa0-\\xa0March 2024\\xa0(2 years 3 months)\\nBengaluru\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\nAnalyzed financial transactions across multiple SWIFT message types to\\nsupport AML and compliance monitoring, building scalable data pipelines\\nusing HDFS and Hive. Developed 10+ Tableau dashboards delivering key\\ninsights and implemented RBAC for 50+ users to ensure secure, compliant\\ndata access.\\nState Street\\n2 years 5 months\\nOfficer\\nApril 2021\\xa0-\\xa0December 2021\\xa0(9 months)\\nBangalore Urban\\nDesigned and developed FP&A dashboards using Tableau and Power BI\\nto support financial decision-making, automated 5+ legacy Excel reports\\nto improve accuracy and efficiency, managed RBAC for global users, and\\nprovided production support to ensure reliable reporting performance.\\nSenior Associate\\nAugust 2019\\xa0-\\xa0March 2021\\xa0(1 year 8 months)\\nBangalore Urban, Karnataka, India\\nEnquero\\nEngineer -Data Analysis and Visualization\\nFebruary 2019\\xa0-\\xa0August 2019\\xa0(7 months)\\nBangalore\\nDeveloped and maintained SQL-based ETL workflows to populate EDW\\ntables with automated scheduling, enhanced dashboards by integrating new\\nKPIs while maintaining performance, and provided production support by\\ntroubleshooting data issues and implementing enhancements.\\nTata Consultancy Services\\n6 years 5 months\\nIT Analyst\\nOctober 2016\\xa0-\\xa0February 2019\\xa0(2 years 5 months)\\nBangalore\\nDesigned and developed ETL workflows and EDW data pipelines using\\nSSIS, Informatica, SQL, and Teradata, ensuring data quality and optimizing\\nperformance through workflow tuning. Built and optimized Tableau dashboards\\nand managed Tableau Server administration, including RBAC and site\\ngovernance, to deliver scalable and reliable reporting solutions at American\\nExpress.\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\nSoftware Engineer\\nOctober 2012\\xa0-\\xa0September 2016\\xa0(4 years)\\nBengaluru, Karnataka, India\\nEducation\\nB. V. Bhoomaraddi College of Engg. & Tech., Hubli\\nEngineer's Degree,\\xa0Computer Science\\xa0·\\xa0(2008\\xa0-\\xa02012)\\nKarnataka Science College, Dharwad\\nPre University (PUC),\\xa0Physics, Chemistry, Mathematics,\\nBiology\\xa0·\\xa0(2006\\xa0-\\xa02008)\\nKarnataka High School, Dharwad\\n10,\\xa0English, Sanskrit, Kannada, Mathematics, Science\\xa0·\\xa0(2000\\xa0-\\xa02006)\\nVidyaranya Primary School, Dharwad\\nGrade 4,\\xa0Kannada, Basic Mathematics, Basic Science\\xa0·\\xa0(1996\\xa0-\\xa02000)\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as vhegade.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "\"\"\"\n",
    "Pydantic AI is a Python framework designed to build production-grade, type-safe, \n",
    "and reliable LLM-powered applications, often described as having a \"FastAPI-like\" experience. \n",
    "It leverages Pydantic for strict data validation and structured outputs, making it easier to \n",
    "integrate AI into existing software, supporting models like OpenAI, Anthropic, and Gemini\n",
    "\"\"\"\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.5-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not hold any patents. My focus has primarily been on building scalable data platforms, analytics solutions, and executive dashboards throughout my career. If you have any questions about my work or projects, feel free to ask!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The agent directly and accurately answers the question based on the provided context. It then professionally pivots to its core expertise and invites further questions, maintaining an engaging tone appropriate for the persona.')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "The agent's response is in Pig Latin, which is not professional or engaging for a potential client or future employer. The agent should communicate clearly and directly.\n",
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
